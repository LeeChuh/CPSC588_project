{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the transaction_candidate_list, we want to find transaction with known change revealed by multi-agent heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4087 blocks loaded\n"
     ]
    }
   ],
   "source": [
    "# path of blocks\n",
    "SOURCE_PATH = '../data/txn_data'\n",
    "# SOURCE_PATH = 'testData'\n",
    "# read files \n",
    "block_list = os.listdir(SOURCE_PATH)\n",
    "sorted_block_list = [y for _, y in sorted([(int(a.split('.')[0]), a) for a in block_list])]\n",
    "sorted_block_list = sorted_block_list\n",
    "# print(sorted_block_list[:10])\n",
    "print(f\"There are {len(sorted_block_list)} blocks loaded\")\n",
    "\n",
    "\n",
    "def get_block_txn_list(block_json_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    @input: a block json file, e.g., 1111.json\n",
    "    @output: a list of transaction in that block\n",
    "    \"\"\"\n",
    "    json_path = os.path.join(SOURCE_PATH, block_json_file)\n",
    "    f = open(json_path)\n",
    "    json_data = json.load(f)\n",
    "    tx = json_data['tx']\n",
    "    f.close()\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of transactions with reused addresses issue: 1,231,860\n",
      "number of transactions with overlay application issue: 16,165\n",
      "number of candidate transactions: 4,143,799\n"
     ]
    }
   ],
   "source": [
    "ADDRESS_REUSE_CODE = 0\n",
    "CLUSTER_MEMBER_CODE = 1\n",
    "UNKNOWN_CHANGE_CODE = 2\n",
    "OVERLAY_APPLICATION_CODE = 3\n",
    "TWO_OUTPUT_CODE = 2\n",
    "\n",
    "with open('../heuristic_data/address_reuse_transaction_address_list.pkl', 'rb') as f:\n",
    "    address_reuse_transaction_address_list = pickle.load(f)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('../heuristic_data/overlay_application_transaction_address_list.pkl', 'rb') as f:\n",
    "    overlay_application_transaction_address_list = pickle.load(f)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('../heuristic_data/transaction_candidate_list.pkl', 'rb') as f:\n",
    "    transaction_candidate_list = pickle.load(f)\n",
    "\n",
    "################################################################################################\n",
    "with open('../heuristic_data/transaction_address_summary.json') as f:\n",
    "    transaction_address_summary = json.load(f)\n",
    "\n",
    "################################################################################################\n",
    "with open('../heuristic_data/two_output_address_summary.json') as f:\n",
    "    two_output_address_summary = json.load(f)\n",
    "\n",
    "print(f\"number of transactions with reused addresses issue: {len(address_reuse_transaction_address_list):,}\")\n",
    "print(f\"number of transactions with overlay application issue: {len(overlay_application_transaction_address_list):,}\")\n",
    "print(f\"number of candidate transactions: {len(transaction_candidate_list):,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement multi-agent heuristic clustering method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an address book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_address_():\n",
    "#     \"\"\"\n",
    "#     @return: \n",
    "#     return the sender address and receiver address of all candidate transactions\n",
    "#     \"\"\"\n",
    "#     local_address_book = []\n",
    "#     for block_json_file in tqdm(sorted_block_list):\n",
    "#         # get transaction list of that block\n",
    "#         txns_list = get_block_txn_list(block_json_file)\n",
    "        \n",
    "#         for txn in txns_list:\n",
    "#             txn_hash = txn[\"hash\"]\n",
    "#             if transaction_address_summary[txn_hash] == TWO_OUTPUT_CODE \\\n",
    "#                     and two_output_address_summary[txn_hash] != ADDRESS_REUSE_CODE \\\n",
    "#                     and two_output_address_summary[txn_hash] != OVERLAY_APPLICATION_CODE:\n",
    "                \n",
    "#                 for input_ in txn[\"inputs\"]:\n",
    "#                     local_address_book.append(input_[\"prev_out\"][\"addr\"])\n",
    "                \n",
    "#                 for output in txn[\"out\"]:\n",
    "#                     if \"addr\" in output:\n",
    "#                         local_address_book.append(output['addr'])\n",
    "#     return local_address_book\n",
    "\n",
    "\n",
    "# address_book = list(set(get_all_address_()))\n",
    "# \"\"\"address_book is a list of the address of all senders and receivers\"\"\"\n",
    "# print(f\"There are {len(address_book):,} candidate address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11,341,115 candidate address\n"
     ]
    }
   ],
   "source": [
    "# with open('../heuristic_data/address_book.pkl', 'wb') as f:\n",
    "#     pickle.dump(address_book, f)\n",
    "\n",
    "with open('../heuristic_data/address_book.pkl', 'rb') as f:\n",
    "    address_book = pickle.load(f)\n",
    "\n",
    "print(f\"There are {len(address_book):,} candidate address\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize the clustering dictionary of addresses in transactions in transaction_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11341115/11341115 [00:03<00:00, 2857364.67it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_agent_input_clustering = {}\n",
    "\"\"\"\n",
    "key: transaction hash of each transaction in transaction_candidate_list\n",
    "value: clustering id; if there's no clustering assigned, clustering id is -1\n",
    "\"\"\"\n",
    "for address in tqdm(address_book):\n",
    "    multi_agent_input_clustering[address] = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many times an input / output address appeared in candidate transaction\n",
    "- Store results in count_clustering_candidate hash map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_clustering_candidate = {}\n",
    "\n",
    "# # initialize the clustering hash table\n",
    "# for address in tqdm(address_book):\n",
    "#     count_clustering_candidate[address] = 0\n",
    "\n",
    "# for block_json_file in tqdm(sorted_block_list):\n",
    "#     # get transaction list of that block\n",
    "#     txns_list = get_block_txn_list(block_json_file)\n",
    "\n",
    "#     for txn in txns_list:\n",
    "#         txn_hash = txn[\"hash\"]\n",
    "#         if transaction_address_summary[txn_hash] == TWO_OUTPUT_CODE \\\n",
    "#                 and two_output_address_summary[txn_hash] != ADDRESS_REUSE_CODE \\\n",
    "#                 and two_output_address_summary[txn_hash] != OVERLAY_APPLICATION_CODE:\n",
    "#             input_list = []\n",
    "#             output_list = []\n",
    "#             for input_ in txn[\"inputs\"]:\n",
    "#                 input_addr = input_[\"prev_out\"][\"addr\"]\n",
    "#                 count_clustering_candidate[input_addr] += 1\n",
    "\n",
    "#             for output in txn[\"out\"]:\n",
    "#                 if \"addr\" in output:\n",
    "#                     output_addr = output['addr']\n",
    "#                     count_clustering_candidate[output_addr] += 1\n",
    "\n",
    "\n",
    "# with open('../heuristic_data/count_clustering_candidate.json', 'w') as f:\n",
    "#     json.dump(count_clustering_candidate, f)\n",
    "\n",
    "with open('../heuristic_data/count_clustering_candidate.json') as f:\n",
    "    count_clustering_candidate = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Merging all inputs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11341115/11341115 [00:03<00:00, 3516849.55it/s]\n"
     ]
    }
   ],
   "source": [
    "clustering_candidate = {}\n",
    "change_address_reuse_list = []\n",
    "\n",
    "# initialize the clustering hash table\n",
    "for address in tqdm(address_book):\n",
    "    clustering_candidate[address] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4087/4087 [07:00<00:00,  9.71it/s]\n"
     ]
    }
   ],
   "source": [
    "count_lonely = 0\n",
    "current_clustering_index = 0\n",
    "\n",
    "for block_json_file in tqdm(sorted_block_list):\n",
    "    # get transaction list of that block\n",
    "    txns_list = get_block_txn_list(block_json_file)\n",
    "    \n",
    "    for txn in txns_list:\n",
    "        txn_hash = txn[\"hash\"]\n",
    "        if transaction_address_summary[txn_hash] == TWO_OUTPUT_CODE \\\n",
    "                and two_output_address_summary[txn_hash] != ADDRESS_REUSE_CODE \\\n",
    "                and two_output_address_summary[txn_hash] != OVERLAY_APPLICATION_CODE:\n",
    "            \n",
    "            input_list = []\n",
    "            output_list = []\n",
    "\n",
    "            ######################################################################\n",
    "            # store inputs and outputs\n",
    "            for input_ in txn[\"inputs\"]:\n",
    "                input_list.append(input_[\"prev_out\"][\"addr\"])\n",
    "            for output in txn[\"out\"]:\n",
    "                if \"addr\" in output:\n",
    "                    output_list.append(output[\"addr\"])\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # check if none of its input/output appear anywhere else\n",
    "            # lonely bit = 1 if it appears anywhere else\n",
    "            lonely_bit = 0\n",
    "            for address in set(input_list).union(set(output_list)):\n",
    "                if count_clustering_candidate[address] != 1:\n",
    "                    lonely_bit = 1\n",
    "                    break\n",
    "            \n",
    "            if not lonely_bit:\n",
    "                count_lonely += 1\n",
    "                continue\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            explored_bit = 0  # 1 if any of the input is explored before \n",
    "            explored_cluster_id_ = None\n",
    "            for input_ in input_list:\n",
    "                if clustering_candidate[input_] != -1:\n",
    "                    explored_bit = 1\n",
    "                    explored_cluster_id_ = clustering_candidate[input_]\n",
    "                    break\n",
    "            ######################################################################\n",
    "\n",
    "            if explored_bit:\n",
    "                for input_ in input_list:\n",
    "                    clustering_candidate[input_] = explored_cluster_id_\n",
    "                # check if it's discovered before\n",
    "                for output in output_list:\n",
    "                    if clustering_candidate[output] == explored_cluster_id_:\n",
    "                        change_address_reuse_list.append(txn_hash)\n",
    "                        break\n",
    "            else:\n",
    "                for input_ in input_list:\n",
    "                    clustering_candidate[input_] = current_clustering_index\n",
    "                current_clustering_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../heuristic_data/clustering_candidate.json', 'w') as f:\n",
    "    json.dump(clustering_candidate, f)\n",
    "\n",
    "# with open('../heuristic_data/clustering_candidate.json') as f:\n",
    "#     clustering_candidate = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter transactions where the change has been revealed by the multi-input heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_member_list = []\n",
    "unknown_change_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4087/4087 [07:55<00:00,  8.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for block_json_file in tqdm(sorted_block_list):\n",
    "    # get transaction list of that block\n",
    "    txns_list = get_block_txn_list(block_json_file)\n",
    "    \n",
    "    for txn in txns_list:\n",
    "        txn_hash = txn[\"hash\"]\n",
    "        if transaction_address_summary[txn_hash] == TWO_OUTPUT_CODE \\\n",
    "                and two_output_address_summary[txn_hash] != ADDRESS_REUSE_CODE \\\n",
    "                and two_output_address_summary[txn_hash] != OVERLAY_APPLICATION_CODE:\n",
    "            \n",
    "            input_list = []\n",
    "            output_list = []\n",
    "            \n",
    "            ######################################################################\n",
    "            # store inputs and outputs\n",
    "            for input_ in txn[\"inputs\"]:\n",
    "                input_list.append(input_[\"prev_out\"][\"addr\"])\n",
    "            for output in txn[\"out\"]:\n",
    "                if \"addr\" in output:\n",
    "                    output_list.append(output[\"addr\"])\n",
    "            ######################################################################\n",
    "\n",
    "            input_cluster_id = clustering_candidate[input_[\"prev_out\"][\"addr\"]]\n",
    "            clustering_member_bit = 0\n",
    "            for output_addr in output_list:\n",
    "                if clustering_candidate[output_addr] != -1 and clustering_candidate[output_addr] == input_cluster_id:\n",
    "                    clustering_member_bit = 1\n",
    "                    break\n",
    "            \n",
    "            if clustering_member_bit:\n",
    "                clustering_member_list.append(txn_hash)\n",
    "            else:\n",
    "                unknown_change_list.append(txn_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140,828 cluster member transactions\n",
      "There are 4,002,971 unknown change transaction\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(clustering_member_list):,} cluster member transactions\")\n",
    "print(f\"There are {len(unknown_change_list):,} unknown change transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../heuristic_data/clustering_member_list.pkl', 'wb') as f:\n",
    "    pickle.dump(clustering_member_list, f)\n",
    "\n",
    "with open('../heuristic_data/unknown_change_list.pkl', 'wb') as f:\n",
    "    pickle.dump(unknown_change_list, f)\n",
    "\n",
    "with open('../heuristic_data/clustering_candidate.json', 'w') as f:\n",
    "    json.dump(clustering_candidate, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140828"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clustering_member_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../heuristic_data/change_address_reuse_list.pkl', 'wb') as f:\n",
    "    pickle.dump(change_address_reuse_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
